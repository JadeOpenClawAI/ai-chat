# AI Chat

A production-quality, full-stack AI chat application built with Next.js 15, the Vercel AI SDK, and @assistant-ui/react.

![AI Chat Interface](docs/screenshot-placeholder.png)

---

## âœ¨ Features

- **Multi-provider LLM support** â€” Anthropic Claude + OpenAI in one app, switchable per conversation
- **Streaming responses** â€” real-time token streaming via Vercel AI SDK data stream protocol
- **Multi-modal input** â€” drag & drop images, PDFs, text files, and videos
- **Tool calling with progress UI** â€” animated state indicators (pending â†’ running â†’ summarizing â†’ done)
- **Automatic context management** â€” tracks token usage, auto-compacts old messages using AI summarization when approaching limits
- **Tool result summarization** â€” oversized tool outputs are automatically condensed to save context
- **Context stats bar** â€” live token counter with visual usage meter
- **Model/provider selector** â€” switch between Claude and GPT models without reloading
- **Dark mode support** â€” system preference aware

---

## ğŸš€ Quick Start

### 1. Clone and install

```bash
git clone <repo-url>
cd ai-chat
pnpm install
```

### 2. Configure environment

```bash
cp .env.example .env.local
```

Edit `.env.local` and fill in at least one API key:

```env
ANTHROPIC_API_KEY=sk-ant-...   # For Claude models
OPENAI_API_KEY=sk-...          # For GPT models
```

### 3. Run development server

```bash
pnpm dev
```

Open [https://localhost:1455](https://localhost:1455) â€” the chat interface loads immediately.
By default, dev uses a self-signed localhost cert generated locally (no sudo required). Set `NO_SELF_SIGNED_CERT=1` to disable that default.
Generated certs are cached at `~/.ai-chat/dev-certs` (override with `DEV_GENERATED_CERT_DIR`) and regenerated automatically when expired.

---

## ğŸ“¦ Architecture Decisions

### AI Framework: Vercel AI SDK (`ai` v4)

**Why:** After evaluating LangChain.js, LangGraph.js, Mastra, and the Vercel AI SDK:

| Framework | Weekly Downloads | TypeScript | Streaming | Tool Calling | Multi-Modal |
|-----------|-----------------|------------|-----------|--------------|-------------|
| **Vercel AI SDK** | ~4M | âœ… Excellent | âœ… Native | âœ… Native | âœ… Native |
| LangChain.js | ~800K | âœ… Good | âš  Adapter | âœ… Native | âš  Manual |
| LangGraph.js | ~150K | âœ… Good | âš  Complex | âœ… Native | âš  Manual |
| Mastra | ~10K | âœ… Good | âš  Limited | âœ… Native | âš  Manual |

The Vercel AI SDK's `streamText` + `toDataStreamResponse` pattern is purpose-built for this use case, has excellent TypeScript types, and integrates natively with Next.js App Router.

### UI Library: @assistant-ui/react

**Why:** Built specifically for the Vercel AI SDK. Key features:
- Native integration with `useChat` hook via `useVercelUseChatRuntime`
- Tool call visualization primitives
- TypeScript-first, well-maintained
- Composable (doesn't force opinions on styling)

### Other key choices:
- **pnpm** â€” faster installs, strict dependency management
- **Next.js 15 App Router** â€” streaming RSC, file-based routing, edge-compatible
- **Tailwind CSS** â€” utility-first, no runtime overhead
- **Zod** â€” schema validation for API requests

---

## ğŸ§  Context Management

The context manager (`lib/ai/context-manager.ts`) runs on every API request:

```
Request received
       â”‚
       â–¼
  Count tokens in messages + system prompt
       â”‚
       â–¼
  Used >= COMPACTION_THRESHOLD (default 75%)?
       â”‚
     Yes â”‚ No
       â–¼   â””â”€â”€â–º Continue with original messages
  Apply compaction mode:
  - truncate: drop oldest messages
  - summary: AI summarize old history into one summary message
  - running-summary: maintain/update a rolling AI summary
       â”‚
       â–¼
  Reduce context toward COMPACTION_TARGET_RATIO
       â”‚
       â–¼
  Stream response with context metadata (wasCompacted/mode/tokensFreed)
```

**Configuration:**
```env
MAX_CONTEXT_TOKENS=150000      # Token budget
CONTEXT_COMPACTION_MODE=summary  # off | truncate | summary | running-summary
COMPACTION_THRESHOLD=0.75        # Start compacting at 75%
COMPACTION_TARGET_RATIO=0.10     # Compact down near 10%
KEEP_RECENT_MESSAGES=10          # Keep last N messages verbatim if possible
MIN_RECENT_MESSAGES=4            # Absolute minimum recent messages to keep
RUNNING_SUMMARY_THRESHOLD=0.35   # Extra trigger used by running-summary mode
```

### Tool Result Summarization

Large tool results are compacted before being added to context (configurable via Settings):

```env
TOOL_COMPACTION_MODE=summary          # off | summary | truncate
TOOL_COMPACTION_THRESHOLD=2000        # Token threshold
TOOL_COMPACTION_SUMMARY_MAX_TOKENS=1000
TOOL_COMPACTION_INPUT_MAX_CHARS=50000
TOOL_COMPACTION_TRUNCATE_MAX_CHARS=8000
```

The UI shows a âš¡ "Summarized" badge on tool calls whose results were condensed.

---

## ğŸ”§ Adding Custom Tools

### 1. Define the tool (in `lib/tools/examples.ts` or a new file)

```typescript
import { tool } from 'ai'
import { z } from 'zod'

export const myCustomTool = tool({
  description: 'What this tool does',
  parameters: z.object({
    input: z.string().describe('The input parameter'),
  }),
  execute: async ({ input }) => {
    // Your tool logic here
    return { result: `Processed: ${input}` }
  },
})
```

### 2. Register it in the tool map

In `lib/tools/examples.ts`, add to `ALL_TOOLS`:

```typescript
export const ALL_TOOLS = {
  // ... existing tools
  myCustomTool,
}

export const TOOL_METADATA = {
  // ... existing metadata
  myCustomTool: {
    icon: 'ğŸ› ',
    description: 'My custom tool',
    expectedDurationMs: 1000,
  },
}
```

### 3. The tool is automatically:
- Exposed to the LLM via the `/api/chat` route
- Tracked with state updates (pending â†’ running â†’ done)
- Listed in `/api/tools`
- Displayed with progress UI in the chat

---

## ğŸ“ Project Structure

```
ai-chat/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/route.ts        # POST: streaming chat; GET: model list
â”‚   â”‚   â””â”€â”€ tools/route.ts       # GET: available tools
â”‚   â”œâ”€â”€ globals.css
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ page.tsx                 # Entry point â†’ <ChatInterface />
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx    # Main layout: header + messages + input
â”‚   â”‚   â”œâ”€â”€ MessageList.tsx      # Message bubbles, markdown, tool calls
â”‚   â”‚   â”œâ”€â”€ MessageInput.tsx     # Textarea + drag&drop + send/stop
â”‚   â”‚   â”œâ”€â”€ ToolCallProgress.tsx # Tool state: pending/running/done/error
â”‚   â”‚   â””â”€â”€ FilePreview.tsx      # File thumbnail + attachment list
â”‚   â””â”€â”€ providers/
â”‚       â””â”€â”€ AIProvider.tsx       # @assistant-ui/react runtime wrapper
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useChat.ts               # Extended Vercel AI SDK useChat
â”‚   â”œâ”€â”€ useTokenCounter.ts       # Client-side token estimation
â”‚   â””â”€â”€ useFileUpload.ts         # File processing + base64 conversion
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ providers.ts         # Claude + OpenAI model factories
â”‚   â”‚   â”œâ”€â”€ tools.ts             # Tool bundle for API route
â”‚   â”‚   â”œâ”€â”€ context-manager.ts   # Token counting + conversation compaction
â”‚   â”‚   â”œâ”€â”€ summarizer.ts        # Tool result summarization
â”‚   â”‚   â””â”€â”€ streaming.ts         # Stream annotation helpers
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ registry.ts          # ToolRegistry class with state tracking
â”‚   â”‚   â””â”€â”€ examples.ts          # Built-in tools (calculator, web search, etc.)
â”‚   â”œâ”€â”€ types.ts                 # Shared TypeScript interfaces
â”‚   â””â”€â”€ utils.ts                 # cn(), formatBytes(), formatTokens()
â”œâ”€â”€ .env.example                 # Environment variable template
â”œâ”€â”€ next.config.ts
â”œâ”€â”€ tailwind.config.ts
â””â”€â”€ tsconfig.json
```

---

## ğŸŒ API Reference

### `POST /api/chat`

Streams a chat response.

**Request body:**
```json
{
  "messages": [{ "role": "user", "content": "Hello" }],
  "provider": "anthropic",
  "model": "claude-sonnet-4-5",
  "systemPrompt": "You are a helpful assistant."
}
```

**Response headers:**
- `X-Context-Used` â€” estimated tokens used
- `X-Context-Limit` â€” configured token limit
- `X-Was-Compacted` â€” `"true"` if conversation was auto-compacted
- `X-Compaction-Configured-Mode` â€” active configured strategy (`off`, `truncate`, `summary`, `running-summary`)
- `X-Compaction-Threshold` â€” active configured threshold ratio
- `X-Compaction-Mode` â€” `truncate | summary | running-summary` when compacted
- `X-Compaction-Tokens-Freed` â€” estimated tokens removed by compaction

**Stream annotations (via Vercel AI SDK data protocol):**
```json
{ "type": "tool-state", "toolCallId": "...", "toolName": "webSearch", "state": "done", "resultSummarized": false }
{ "type": "context-stats", "used": 12000, "limit": 150000, "percentage": 0.08, "wasCompacted": false, "compactionMode": "summary", "tokensFreed": 9321 }
```

### `GET /api/chat`

Returns available models based on configured API keys.

### `GET /api/tools`

Returns tool definitions with metadata (name, description, icon, expectedDurationMs).

---

## ğŸ”‘ Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ANTHROPIC_API_KEY` | â€” | Anthropic API key |
| `OPENAI_API_KEY` | â€” | OpenAI API key |
| `DEFAULT_PROVIDER` | `anthropic` | Active provider |
| `DEFAULT_MODEL` | `claude-sonnet-4-5` | Active model |
| `MAX_CONTEXT_TOKENS` | `150000` | Token budget |
| `CONTEXT_COMPACTION_MODE` | `summary` | Context strategy (`off`, `truncate`, `summary`, `running-summary`) |
| `COMPACTION_THRESHOLD` | `0.75` | Start compacting at this fraction |
| `COMPACTION_TARGET_RATIO` | `0.10` | Target fraction after compaction |
| `KEEP_RECENT_MESSAGES` | `10` | Messages kept verbatim during compaction |
| `MIN_RECENT_MESSAGES` | `4` | Minimum recent messages kept if still oversized |
| `RUNNING_SUMMARY_THRESHOLD` | `0.35` | Running-summary refresh threshold |
| `COMPACTION_SUMMARY_MAX_TOKENS` | `1200` | Max generated tokens for conversation summary |
| `COMPACTION_TRANSCRIPT_MAX_CHARS` | `120000` | Max chars sent to compaction summarizer |
| `TOOL_COMPACTION_MODE` | `summary` | Tool-result strategy (`off`, `summary`, `truncate`) |
| `TOOL_COMPACTION_THRESHOLD` | `2000` | Token threshold for tool compaction |
| `TOOL_COMPACTION_SUMMARY_MAX_TOKENS` | `1000` | Max generated tokens for tool-result summaries |
| `TOOL_COMPACTION_INPUT_MAX_CHARS` | `50000` | Max tool-output chars provided to summarizer |
| `TOOL_COMPACTION_TRUNCATE_MAX_CHARS` | `8000` | Max chars retained in truncate mode/fallback |
| `TOOL_RESULT_SUMMARY_THRESHOLD` | `2000` | Legacy alias for `TOOL_COMPACTION_THRESHOLD` |
| `NEXT_PUBLIC_APP_NAME` | `AI Chat` | App title shown in UI |
| `NO_SELF_SIGNED_CERT` | unset | Disable default self-signed HTTPS in `pnpm dev` (local cert generation) |
| `NO_HTTP_TO_HTTPS_REDIRECT` | unset | Disable default same-port HTTP->HTTPS redirect in `pnpm dev` |
| `DEV_HTTPS_KEY_FILE` | unset | Optional custom HTTPS key file path for `pnpm dev` |
| `DEV_HTTPS_CERT_FILE` | unset | Optional custom HTTPS cert file path for `pnpm dev` |
| `DEV_HTTPS_CA_FILE` | unset | Optional custom HTTPS CA file path for `pnpm dev` |
| `DEV_GENERATED_CERT_DIR` | `~/.ai-chat/dev-certs` | Cache dir for generated localhost cert/key; reused until expiry |

---

## ğŸ›  Development

```bash
pnpm dev          # Start dev server (https://localhost:1455)
pnpm build        # Production build
pnpm start        # Start production server
pnpm lint         # ESLint
pnpm type-check   # TypeScript check
```

---

## ğŸ“„ License

MIT
